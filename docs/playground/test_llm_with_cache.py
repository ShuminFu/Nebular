"""ç”¨äºæµ‹è¯•LLMç”Ÿæˆçš„ç¼“å­˜æœºåˆ¶ã€‚
åŸºäºURLç¼“å­˜æ”¹é€ ï¼Œæ”¯æŒå¯¹LLMè¯·æ±‚å“åº”çš„ç¼“å­˜ï¼Œé¿å…é‡å¤è°ƒç”¨APIã€‚
"""

import unittest
from unittest.mock import patch
import logging
import json
import os
from datetime import datetime, timedelta
import hashlib

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LLMCache:
    """ä¸“é—¨ç”¨äºç¼“å­˜LLMå“åº”çš„ç¼“å­˜ç±»"""

    def __init__(self, ttl_seconds=3600, cache_dir=None):
        self.ttl_seconds = ttl_seconds
        # è®¾ç½®ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤åœ¨å½“å‰æ–‡ä»¶ç›®å½•ä¸‹çš„ .llm_cache
        current_file_dir = os.path.dirname(os.path.abspath(__file__))
        self.cache_dir = cache_dir or os.path.join(current_file_dir, '.llm_cache')
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(self.cache_dir, exist_ok=True)
        logger.info(f"ä½¿ç”¨LLMç¼“å­˜ç›®å½•: {self.cache_dir}")

    def _generate_cache_key(self, model: str, messages: list, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®
        
        è€ƒè™‘æ‰€æœ‰å¯èƒ½å½±å“LLMè¾“å‡ºçš„å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š
        - æ¨¡å‹åç§°
        - æ¶ˆæ¯å†…å®¹
        - å…¶ä»–å‚æ•°(temperature, top_pç­‰)
        """
        # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰å‚æ•°çš„å­—å…¸
        cache_dict = {
            'model': model,
            'messages': messages,
            **kwargs
        }
        # å°†å­—å…¸è½¬æ¢ä¸ºè§„èŒƒåŒ–çš„JSONå­—ç¬¦ä¸²
        cache_str = json.dumps(cache_dict, sort_keys=True)
        # ä½¿ç”¨MD5ç”Ÿæˆç¼“å­˜é”®
        return hashlib.md5(cache_str.encode()).hexdigest()

    def _get_cache_path(self, cache_key: str) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self.cache_dir, f"{cache_key}.json")

    def get(self, model: str, messages: list, **kwargs) -> dict:
        """è·å–ç¼“å­˜çš„LLMå“åº”
        
        Args:
            model: æ¨¡å‹åç§°
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°(temperature, top_pç­‰)
            
        Returns:
            dict: ç¼“å­˜çš„å“åº”æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰ç¼“å­˜åˆ™è¿”å›None
        """
        cache_key = self._generate_cache_key(model, messages, **kwargs)
        cache_path = self._get_cache_path(cache_key)

        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    cached_data = json.load(f)
                    # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                    if datetime.fromisoformat(cached_data['expires']) > datetime.now():
                        logger.info(f"ğŸ¯ å‘½ä¸­LLMç¼“å­˜: {model}")
                        return cached_data['data']
                    else:
                        logger.info(f"âŒ› LLMç¼“å­˜è¿‡æœŸ: {model}")
                        os.remove(cache_path)
            except (json.JSONDecodeError, KeyError, OSError) as e:
                logger.warning(f"è¯»å–LLMç¼“å­˜å¤±è´¥: {e}")
                if os.path.exists(cache_path):
                    os.remove(cache_path)
        return None

    def set(self, model: str, messages: list, response_data: dict, **kwargs):
        """è®¾ç½®LLMå“åº”ç¼“å­˜
        
        Args:
            model: æ¨¡å‹åç§°
            messages: æ¶ˆæ¯åˆ—è¡¨
            response_data: å“åº”æ•°æ®
            **kwargs: å…¶ä»–å‚æ•°(temperature, top_pç­‰)
        """
        cache_key = self._generate_cache_key(model, messages, **kwargs)
        cache_path = self._get_cache_path(cache_key)

        cache_data = {
            'data': response_data,
            'expires': (datetime.now() + timedelta(seconds=self.ttl_seconds)).isoformat()
        }

        try:
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ å·²ç¼“å­˜LLMå“åº”: {model} -> {cache_path}")
        except OSError as e:
            logger.error(f"å†™å…¥LLMç¼“å­˜å¤±è´¥: {e}")

    def clear(self):
        """æ¸…é™¤æ‰€æœ‰LLMç¼“å­˜"""
        for file in os.listdir(self.cache_dir):
            if file.endswith('.json'):
                os.remove(os.path.join(self.cache_dir, file))
        logger.info("ğŸ§¹ å·²æ¸…é™¤æ‰€æœ‰LLMç¼“å­˜")


# åˆ›å»ºå…¨å±€LLMç¼“å­˜å®ä¾‹
llm_cache = LLMCache()


def call_llm_with_cache(model: str, messages: list, use_cache=True, **kwargs) -> dict:
    """è°ƒç”¨LLMæ¥å£ï¼Œæ”¯æŒç¼“å­˜
    
    Args:
        model: æ¨¡å‹åç§°
        messages: æ¶ˆæ¯åˆ—è¡¨
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤True
        **kwargs: å…¶ä»–å‚æ•°(temperature, top_pç­‰)
        
    Returns:
        dict: LLMå“åº”æ•°æ®
    """
    if use_cache:
        cached_response = llm_cache.get(model, messages, **kwargs)
        if cached_response:
            return cached_response

    # è¿™é‡Œæ›¿æ¢ä¸ºå®é™…çš„LLMè°ƒç”¨
    # ç¤ºä¾‹ï¼šresponse = openai.ChatCompletion.create(model=model, messages=messages, **kwargs)
    response = {
        'model': model,
        'choices': [
            {
                'message': {
                    'role': 'assistant',
                    'content': 'This is a mock response'
                }
            }
        ]
    }

    if use_cache:
        llm_cache.set(model, messages, response, **kwargs)

    return response


class TestLLMWithCache(unittest.TestCase):
    """æµ‹è¯•å¸¦ç¼“å­˜çš„LLMè°ƒç”¨"""

    def setUp(self):
        """æµ‹è¯•å¼€å§‹å‰æ¸…ç†ç¼“å­˜"""
        llm_cache.clear()

    def test_llm_cache(self):
        """æµ‹è¯•LLMç¼“å­˜åŠŸèƒ½"""
        model = "gpt-3.5-turbo"
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello!"}
        ]

        # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆåº”è¯¥å®é™…è°ƒç”¨APIï¼‰
        logger.info("æµ‹è¯•LLMç¼“å­˜ - ç¬¬ä¸€æ¬¡è°ƒç”¨")
        result1 = call_llm_with_cache(model, messages, temperature=0.7)
        self.assertIn('choices', result1)

        # éªŒè¯ç¼“å­˜æ–‡ä»¶å·²åˆ›å»º
        cache_key = llm_cache._generate_cache_key(model, messages, temperature=0.7)
        cache_file = llm_cache._get_cache_path(cache_key)
        self.assertTrue(os.path.exists(cache_file))

        # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆåº”è¯¥ä½¿ç”¨ç¼“å­˜ï¼‰
        logger.info("æµ‹è¯•LLMç¼“å­˜ - ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆåº”è¯¥ä½¿ç”¨ç¼“å­˜ï¼‰")
        result2 = call_llm_with_cache(model, messages, temperature=0.7)
        self.assertEqual(result1, result2)

        # æµ‹è¯•ä¸åŒå‚æ•°ç”Ÿæˆä¸åŒçš„ç¼“å­˜é”®
        logger.info("æµ‹è¯•ä¸åŒå‚æ•°çš„ç¼“å­˜é”®")
        result3 = call_llm_with_cache(model, messages, temperature=0.8)
        cache_key2 = llm_cache._generate_cache_key(model, messages, temperature=0.8)
        self.assertNotEqual(cache_key, cache_key2)

        # æµ‹è¯•ç¦ç”¨ç¼“å­˜
        logger.info("æµ‹è¯•ç¦ç”¨ç¼“å­˜")
        result4 = call_llm_with_cache(model, messages, use_cache=False)
        self.assertIn('choices', result4)


if __name__ == '__main__':
    unittest.main(verbosity=2)
